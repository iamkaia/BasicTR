{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00 - 5.34] 目前確定一定會有,其實之後我會\n",
      "[5.34 - 10.54] 然後另外的話,我這邊會有木柯斯跟望紅那邊的\n",
      "[10.54 - 15.86] 但是望紅那邊的話,基本上就是用公路上的方式\n",
      "[15.86 - 19.56] 所以如果你們有醫院接望紅公路生,也是可以的\n",
      "[19.56 - 23.32] 目前的聖經,我幾百預期是二人\n",
      "[23.32 - 26.06] 但是如果你們有人,你覺得自己有能力\n",
      "[27.06 - 31.06] 一進學姐裡面,有接觸到,看過她工作的樣子\n",
      "[31.06 - 35.06] 其實她,聽說,我一直不小心讓她這個黃工作的太累了\n",
      "[35.06 - 37.06] 所以,就是那個東西\n",
      "[37.06 - 42.06] 但是,因為從木柯斯這邊的愛的話,因為你又安排一些\n",
      "[42.06 - 45.06] 講住學期的,住這邊的錢\n",
      "[45.06 - 50.06] 但是,目前需要幫忙的\n",
      "[50.06 - 56.06] 應該主要就是望紅的地方,望紅的地方\n",
      "[56.06 - 60.06] 然後另外一個,可能UVIDA這邊會需要一些幫忙\n",
      "[60.06 - 64.06] 但是,目前來說好像暫時沒有太多需要工作的東西\n",
      "[64.06 - 66.06] 那之後可能再請大家幫忙\n",
      "[66.06 - 70.06] 但,因為這邊主要現在做的大部分都是多麼太累的東西\n",
      "[70.06 - 74.06] 所以,幫忙,這也幫忙做的就是訓練一些\n",
      "[75.06 - 79.06] 如果是那個大法,那時候那個時候是大法\n",
      "[79.06 - 85.06] 有VN的東西,還有一個miniGVTG2的這類東西\n",
      "[85.06 - 89.06] 所以,之後有需要的話會再跟大家講\n",
      "[0.00 - 2.90]  In fact I will come and question details18\n",
      "[2.90 - 5.56]  I will come and answer the most of questions\n",
      "[8.76 - 12.28]  Besides, this man Ram��들\n",
      "[12.60 - 15.62]  is usually working in law\n",
      "[15.72 - 19.26]  Is case with Ram Francisco\n",
      "[19.26 - 22.06] udo can be identified in strong chess\n",
      "[22.06 - 23.08]  I was in 780\n",
      "[23.26 - 27.54]  There is any person in La May\n",
      "[27.54 - 29.78]  Still in school\n",
      "[30.18 - 36.40]  It depends on how he works and how he cuts it off from Workshop punto,\n",
      "[36.40 - 40.26]  so how to changeaire ways.\n",
      "[40.26 - 45.30]  The more important we are, the further the conversion of craziest things forth,\n",
      "[45.30 - 48.30]  and therekward halved possibility\n",
      "[48.32 - 51.06]  in terms of manufacturing day,\n",
      "[51.06 - 54.14]  what we had to monitor for picked colors\n",
      "[54.16 - 57.60]  This is what we decided to do with night.\n",
      "[57.60 - 59.60]  You may need help from the UBDA\n",
      "[59.60 - 63.60]  But if you are like a standing-up, you don't need to do much work\n",
      "[63.60 - 65.60]  Then you can ask for help\n",
      "[65.60 - 69.60]  But the main thing is that you are doing a lot of things\n",
      "[69.60 - 71.60]  So help the help of this help\n",
      "[71.60 - 73.60]  is to train some\n",
      "[73.60 - 75.60]  VR\n",
      "[75.60 - 77.60]  And it's the way\n",
      "[77.60 - 79.60]  That's when you are using the way\n",
      "[79.60 - 80.60]  VR and VR\n",
      "[80.60 - 83.60]  And then there is a mini-GVT\n",
      "[83.60 - 85.60]  G2 is the most important thing\n",
      "[85.60 - 87.60]  So you need to put this on the screen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(result_1[\"text\"])\\nprint(result_2[\"text\"])\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is use LLM\n",
    "import whisper\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 加载 Whisper 模型\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# 载入音频文件并进行转录\n",
    "result_1 = model.transcribe(\"audio_0924_2(project_intro).m4a\", language=\"Chinese\")\n",
    "result_2 = model.transcribe(\"audio_0924_2(project_intro).m4a\", task=\"translate\")\n",
    "# 打印识别出来的文本\n",
    "for segment in result_1[\"segments\"]:\n",
    "    start = segment[\"start\"]\n",
    "    end = segment[\"end\"]\n",
    "    text = segment[\"text\"]\n",
    "    print(f\"[{start:.2f} - {end:.2f}] {text}\")\n",
    "for segment in result_2[\"segments\"]:\n",
    "    start = segment[\"start\"]\n",
    "    end = segment[\"end\"]\n",
    "    text = segment[\"text\"]\n",
    "    print(f\"[{start:.2f} - {end:.2f}] {text}\")\n",
    "'''\n",
    "print(result_1[\"text\"])\n",
    "print(result_2[\"text\"])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
