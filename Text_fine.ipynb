{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#  unsloth_env enviorment\n",
    "from unsloth import FastLanguageModel\n",
    "from torch.amp import autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.3: Fast Llama patching. Transformers = 4.45.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2080 Ti. Max memory: 10.564 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
      "Please update transformers via:\n",
      "`pip uninstall transformers -y && pip install --upgrade --no-cache-dir \"git+https://github.com/huggingface/transformers.git\"`\n"
     ]
    }
   ],
   "source": [
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    #dtype =torch.float32\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"pretrain_model/unsloth/Meta-Llama-3.1-8B-bnb-4bit\") # Local saving\n",
    "#tokenizer.save_pretrained(\"pretrain_tokenizer/unsloth/Meta-Llama-3.1-8B-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Question_list = []\n",
    "def prepare_cornell_dataset(file_path):\n",
    "    # 读取电影台词数据\n",
    "    lines = {}\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            lines[parts[0]] = parts[-1]  # 提取 lineID 和 句子内容\n",
    "\n",
    "    # 读取对话数据\n",
    "    conversations = []\n",
    "    with open('/home/kaia/BasicTR/cornell movie-dialogs corpus/movie_conversations.txt', 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            # 将字符串的 list 形式转换为真正的列表\n",
    "            line_ids = eval(parts[-1])\n",
    "            for i in range(len(line_ids) - 1):\n",
    "                conversations.append((lines[line_ids[i]], lines[line_ids[i + 1]]))\n",
    "\n",
    "    # 将对话数据保存为一个文本文件，供微调使用\n",
    "    with open(\"prepared_cornell_dialogs.txt\", \"w\") as out_f:\n",
    "        for q, a in conversations:\n",
    "            out_f.write(f\"{q}\\t{a}\\n\")  # 用 tab 分隔问题和回答\n",
    "            #print(f'question: {q}')\n",
    "            #print(f'answer: {a}')\n",
    "            Question_list.append(a)\n",
    "\n",
    "# 调用数据准备函数\n",
    "prepare_cornell_dataset('/home/kaia/BasicTR/cornell movie-dialogs corpus/movie_lines.txt')\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Input:\\n{}\\n\\n### Response:\\n{}\"\"\"\\n\\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\\ndef formatting_prompts_func(examples):\\n    instructions = examples[\"instruction\"]\\n    inputs       = examples[\"input\"]\\n    outputs      = examples[\"output\"]\\n    texts = []\\n    for instruction, input, output in zip(instructions, inputs, outputs):\\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\\n        print(text)\\n        break\\n        texts.append(text)\\n    return { \"text\" : texts, }\\npass\\n\\nfrom datasets import load_dataset\\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\\ndataset = dataset.map(formatting_prompts_func, batched = True,)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        print(text)\n",
    "        break\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 定义特殊的 Alpaca 风格的模板\n",
    "#alpaca_prompt = \"\"\"Below is a conversation between two people. Write a response to complete the conversation.\n",
    "alpaca_prompt = \"\"\"Below is a conversation between two people. Please write a single, non-repetitive response to complete the conversation.\n",
    "\n",
    "### User:\n",
    "{}\n",
    "\n",
    "### Agent:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # 必须添加 EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    Person1 = examples[\"User\"]\n",
    "    Person2 = examples[\"Agent\"]\n",
    "    texts = []\n",
    "    for Person1, Person2 in zip(Person1, Person2):\n",
    "        # 必须添加 EOS_TOKEN，否则生成将永远持续下去\n",
    "        text = alpaca_prompt.format(Person1, Person2) + EOS_TOKEN\n",
    "        #print(text)\n",
    "        #break\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# 加载和准备数据集\n",
    "def prepare_cornell_dataset(file_path):\n",
    "    # 加载电影台词数据集，并进行处理\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": file_path}, split=\"train\")\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"User\": x[\"text\"].split(\"\\t\")[0], \"Agent\": x[\"text\"].split(\"\\t\")[1]},\n",
    "        batched=False\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "# 调用函数加载数据\n",
    "dataset = prepare_cornell_dataset(\"prepared_cornell_dialogs.txt\")\n",
    "'''\n",
    "print(dataset)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"text: {dataset['text'][i]}\")\n",
    "    print(f\"Person 1:{dataset['Person 1'][i]}\")\n",
    "    print(f\"Person 2:{dataset['Person 2'][i]}\")\n",
    "'''\n",
    "\n",
    "# 处理并格式化数据集\n",
    "tokenized_dataset = dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 221616/221616 [00:09<00:00, 24257.57 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length, #for input\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, #four step refresh model param \n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  39314,    374,    264,  10652,   1990,   1403,   1274,     13,\n",
      "           5321,   3350,    264,   3254,     11,   2536,   5621,   7005,   3486,\n",
      "           2077,    311,   4686,    279,  10652,    382,  14711,   7508,    220,\n",
      "             16,    512,   6854,    584,   1304,    420,   4062,     30,    220,\n",
      "          72523,  20991,    735,  17847,    483,    323,  13929,  56859,    527,\n",
      "           3515,    459,  17235,  93840,    788,    586,   1464,     12,    709,\n",
      "            389,    279,  28181,     13,    220,  14077,    382,  14711,   7508,\n",
      "            220,     17,    512,  40914,     13,    220,    358,   1781,    430,\n",
      "            596,    279,   2132,    892,    420,   2305,     13,    220,   3639,\n",
      "            596,    709,   1980,  14711,   7508,    220,     16,    512,     40,\n",
      "           2846,   2133,    311,   3371,    499,   2555,    430,    358]],\n",
      "       device='cuda:0')\n",
      "Below is a conversation between two people. Please write a single, non-repetitive response to complete the conversation.\n",
      "\n",
      "### Person 1:\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "\n",
      "### Person 2:\n",
      "Sure.  I think that's the second time this month.  What's up?\n",
      "\n",
      "### Person 1:\n",
      "I'm going to tell you something that I\n",
      "!!!\n",
      "\n",
      "Sure.  I think that's the second time this month.  What's up?\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 生成文本输出\\noutputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\\n\\n# 解码输出\\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\\nprint(generated_text)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_ans = \" \"\n",
    "#alpaca_prompt = Copied from above\n",
    "# 启用更快的推理模式\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# 使用新的模板来定义输入\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\",  # 这是新的 question（对话问题）\n",
    "            \"\"  # 这是 answer（答案），留空以让模型生成\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "with autocast(device_type='cuda'):\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        #streamer=text_streamer,\n",
    "        max_new_tokens=32,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "    print(outputs)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(text)\n",
    "    print(\"!!!\")\n",
    "    temp = text.split(\"### User:\")[1]\n",
    "    response = temp.split(\"### Agent:\")[0]\n",
    "    before_ans = response\n",
    "    print(response)\n",
    "#_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
    "'''\n",
    "# 生成文本输出\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "# 解码输出\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 221,616 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers and Unsloth!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [00:01<01:10,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3608, 'grad_norm': 0.0, 'learning_rate': 4e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/60 [00:02<01:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1699, 'grad_norm': 0.0, 'learning_rate': 8e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/60 [00:03<00:56,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3804, 'grad_norm': 0.0, 'learning_rate': 0.00012, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/60 [00:04<00:55,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6104, 'grad_norm': 0.0, 'learning_rate': 0.00016, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/60 [00:05<00:54,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9697, 'grad_norm': 0.0, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/60 [00:06<00:54,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1472, 'grad_norm': 0.0, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/60 [00:07<00:53,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0973, 'grad_norm': 0.0, 'learning_rate': 0.00019272727272727274, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8/60 [00:08<00:54,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0643, 'grad_norm': 0.0, 'learning_rate': 0.0001890909090909091, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/60 [00:09<00:52,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.1164, 'grad_norm': 0.0, 'learning_rate': 0.00018545454545454545, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10/60 [00:10<00:51,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9428, 'grad_norm': 0.0, 'learning_rate': 0.00018181818181818183, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [00:11<00:50,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1106, 'grad_norm': 0.0, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/60 [00:12<00:48,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4398, 'grad_norm': 0.0, 'learning_rate': 0.00017454545454545454, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/60 [00:13<00:47,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8625, 'grad_norm': 0.0, 'learning_rate': 0.0001709090909090909, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14/60 [00:14<00:46,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0712, 'grad_norm': 0.0, 'learning_rate': 0.00016727272727272728, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15/60 [00:15<00:44,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6485, 'grad_norm': 0.0, 'learning_rate': 0.00016363636363636366, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16/60 [00:16<00:43,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8492, 'grad_norm': 0.0, 'learning_rate': 0.00016, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 17/60 [00:17<00:43,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2363, 'grad_norm': 0.0, 'learning_rate': 0.00015636363636363637, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 18/60 [00:18<00:42,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3804, 'grad_norm': 0.0, 'learning_rate': 0.00015272727272727275, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 19/60 [00:19<00:43,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0753, 'grad_norm': 0.0, 'learning_rate': 0.0001490909090909091, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 20/60 [00:20<00:42,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0042, 'grad_norm': 0.0, 'learning_rate': 0.00014545454545454546, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [00:21<00:39,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6824, 'grad_norm': 0.0, 'learning_rate': 0.00014181818181818184, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/60 [00:22<00:38,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3509, 'grad_norm': 0.0, 'learning_rate': 0.0001381818181818182, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 23/60 [00:23<00:36,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5798, 'grad_norm': 0.0, 'learning_rate': 0.00013454545454545455, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24/60 [00:24<00:35,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5394, 'grad_norm': 0.0, 'learning_rate': 0.00013090909090909093, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 25/60 [00:25<00:35,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4262, 'grad_norm': 0.0, 'learning_rate': 0.00012727272727272728, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 26/60 [00:26<00:34,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2018, 'grad_norm': 0.0, 'learning_rate': 0.00012363636363636364, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 27/60 [00:27<00:32,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4511, 'grad_norm': 0.0, 'learning_rate': 0.00012, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 28/60 [00:28<00:32,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3262, 'grad_norm': 0.0, 'learning_rate': 0.00011636363636363636, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 29/60 [00:29<00:31,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6945, 'grad_norm': 0.0, 'learning_rate': 0.00011272727272727272, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 30/60 [00:30<00:30,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0983, 'grad_norm': 0.0, 'learning_rate': 0.00010909090909090909, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [00:31<00:29,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.408, 'grad_norm': 0.0, 'learning_rate': 0.00010545454545454545, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 32/60 [00:32<00:27,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5704, 'grad_norm': 0.0, 'learning_rate': 0.00010181818181818181, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 33/60 [00:33<00:27,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7473, 'grad_norm': 0.0, 'learning_rate': 9.818181818181818e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 34/60 [00:34<00:26,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4745, 'grad_norm': 0.0, 'learning_rate': 9.454545454545455e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 35/60 [00:35<00:24,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.293, 'grad_norm': 0.0, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 36/60 [00:36<00:23,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4736, 'grad_norm': 0.0, 'learning_rate': 8.727272727272727e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 37/60 [00:37<00:22,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2169, 'grad_norm': 0.0, 'learning_rate': 8.363636363636364e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 38/60 [00:38<00:21,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1161, 'grad_norm': 0.0, 'learning_rate': 8e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 39/60 [00:39<00:21,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8544, 'grad_norm': 0.0, 'learning_rate': 7.636363636363637e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 40/60 [00:40<00:20,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0644, 'grad_norm': 0.0, 'learning_rate': 7.272727272727273e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [00:41<00:19,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2925, 'grad_norm': 0.0, 'learning_rate': 6.90909090909091e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 42/60 [00:42<00:18,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6014, 'grad_norm': 0.0, 'learning_rate': 6.545454545454546e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 43/60 [00:43<00:17,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6266, 'grad_norm': 0.0, 'learning_rate': 6.181818181818182e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 44/60 [00:44<00:16,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5351, 'grad_norm': 0.0, 'learning_rate': 5.818181818181818e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 45/60 [00:45<00:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5693, 'grad_norm': 0.0, 'learning_rate': 5.4545454545454546e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 46/60 [00:46<00:14,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6537, 'grad_norm': 0.0, 'learning_rate': 5.090909090909091e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 47/60 [00:47<00:13,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6613, 'grad_norm': 0.0, 'learning_rate': 4.7272727272727275e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 48/60 [00:48<00:12,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.188, 'grad_norm': 0.0, 'learning_rate': 4.3636363636363636e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 49/60 [00:49<00:11,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0224, 'grad_norm': 0.0, 'learning_rate': 4e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 50/60 [00:50<00:10,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7673, 'grad_norm': 0.0, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [00:52<00:10,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9194, 'grad_norm': 0.0, 'learning_rate': 3.272727272727273e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 52/60 [00:53<00:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5063, 'grad_norm': 0.0, 'learning_rate': 2.909090909090909e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 53/60 [00:54<00:07,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5771, 'grad_norm': 0.0, 'learning_rate': 2.5454545454545454e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 54/60 [00:55<00:06,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2617, 'grad_norm': 0.0, 'learning_rate': 2.1818181818181818e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 55/60 [00:56<00:05,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2867, 'grad_norm': 0.0, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 56/60 [00:57<00:04,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5986, 'grad_norm': 0.0, 'learning_rate': 1.4545454545454545e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 57/60 [00:58<00:03,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3676, 'grad_norm': 0.0, 'learning_rate': 1.0909090909090909e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 58/60 [00:59<00:02,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6559, 'grad_norm': 0.0, 'learning_rate': 7.272727272727272e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 59/60 [01:00<00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3733, 'grad_norm': 0.0, 'learning_rate': 3.636363636363636e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:01<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9994, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:02<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 62.9039, 'train_samples_per_second': 7.631, 'train_steps_per_second': 0.954, 'train_loss': 4.342836685975393, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a conversation between two people. Please write a single, non-repetitive response to complete the conversation.\n",
      "\n",
      "### Person 1:\n",
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "\n",
      "### Person 2:\n",
      "I'm sorry, but I'm not sure I understand.  Who are they, and why is this so important to you?\n",
      "\n",
      "### Person 1:\n",
      "R\n",
      "!!!\n",
      "\n",
      "I'm sorry, but I'm not sure I understand.  Who are they, and why is this so important to you?\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 生成文本输出\\noutputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\\n\\n# 解码输出\\ngenerated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\\nprint(generated_text)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_ans = \" \"\n",
    "#alpaca_prompt = Copied from above\n",
    "# 启用更快的推理模式\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# 使用新的模板来定义输入\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\",  # 这是新的 question（对话问题）\n",
    "            \"\"  # 这是 answer（答案），留空以让模型生成\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "with autocast(device_type='cuda'):\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        #streamer=text_streamer,\n",
    "        max_new_tokens=32,\n",
    "        use_cache=True\n",
    "    )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(text)\n",
    "    print(\"!!!\")\n",
    "    temp = text.split(\"### Person 2:\")[1]\n",
    "    response = temp.split(\"### Person 1:\")[0]\n",
    "    after_ans = response\n",
    "    print(response)\n",
    "#_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
    "'''\n",
    "# 生成文本输出\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "# 解码输出\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_finetune:\n",
      "I don't think I can make it quick.  You know I'm a sucker for public break-ups.\n",
      "after_finetune:\n",
      "I'm sorry, but I'm not sure I understand.  Who are they, and why is this so important to you?\n",
      "\n",
      "\n",
      "reference:Well, I thought we'd start with pronunciation, if that's okay with you.\n"
     ]
    }
   ],
   "source": [
    "before_ans = \"I don't think I can make it quick.  You know I'm a sucker for public break-ups.\"\n",
    "print(f'before_finetune:\\n{before_ans}')\n",
    "print(f'after_finetune:{after_ans}')\n",
    "print(f'reference:{Question_list[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLEU score: 0.0035971223021582736\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "# 计算 GLEU 分数\n",
    "gleu_score = sentence_gleu(after_ans, Question_list[0])\n",
    "\n",
    "print(f\"GLEU score: {gleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# 计算 BLEU 分数\n",
    "bleu = sacrebleu.corpus_bleu(after_ans, Question_list[0])\n",
    "\n",
    "print(f\"BLEU score: {bleu.score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.save_pretrained(\"movielora_model\") # Local saving\\ntokenizer.save_pretrained(\"movielora_model\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model.save_pretrained(\"movielora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"movielora_model\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.save_pretrained(\"lora_model\") # Local saving\\ntokenizer.save_pretrained(\"lora_model\")\\n# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
