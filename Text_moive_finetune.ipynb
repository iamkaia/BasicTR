{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted to 'cornell_movie_dialogs_corpus'\n"
     ]
    }
   ],
   "source": [
    "#LLM和TR都因為和Tensorflow 有一些問題啊啊啊\n",
    "#use finetune\n",
    "#import unsloth#no.1\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 数据集下载 URL（从官方网站获取）\n",
    "url = \"https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "output_dir = \"cornell_movie_dialogs_corpus\"\n",
    "\n",
    "# 下载并保存 zip 文件\n",
    "response = requests.get(url)\n",
    "zip_file_path = \"cornell_movie_dialogs_corpus.zip\"\n",
    "with open(zip_file_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# 解压 zip 文件\n",
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(output_dir)\n",
    "\n",
    "# 删除 zip 文件\n",
    "os.remove(zip_file_path)\n",
    "\n",
    "print(f\"Dataset downloaded and extracted to '{output_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_lines(file_path):\n",
    "    # 加载并解析 movie_lines.txt 文件\n",
    "    lines = {}\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                lines[parts[0]] = parts[4]  # 保存 lineID 和台词\n",
    "    return lines\n",
    "\n",
    "def load_conversations(file_path, lines):\n",
    "    # 加载并解析 movie_conversations.txt 文件，并将对话拼接起来\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            line_ids = eval(parts[-1])\n",
    "            conversation = [lines[line_id] for line_id in line_ids if line_id in lines]\n",
    "            conversations.append(conversation)\n",
    "    return conversations\n",
    "\n",
    "def prepare_dataset(conversations, output_path):\n",
    "    # 将每段对话拼接为一个连续的文本片段\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for conversation in conversations:\n",
    "            for line in conversation:\n",
    "                f.write(line + \"\\n\")\n",
    "            f.write(\"\\n\")  # 每段对话之间留一个空行\n",
    "\n",
    "# 处理数据\n",
    "lines = load_lines(\"/home/kaia/BasicTR/cornell movie-dialogs corpus/movie_lines.txt\")\n",
    "conversations = load_conversations(\"/home/kaia/BasicTR/cornell movie-dialogs corpus/movie_conversations.txt\", lines)\n",
    "prepare_dataset(conversations, \"prepared_conversations.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, GPT2Tokenizer, GPT2LMHeadModel\n",
    "#from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": \"prepared_conversations.txt\"})\n",
    "    return dataset[\"train\"]\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b61f53fe5a47869058b94aefb60f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m\n\u001b[1;32m     70\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m     71\u001b[0m     model\u001b[38;5;241m=\u001b[39mfinetuner\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     72\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     73\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# 因为我们重写了 get_train_dataloader，不需要传递 train_dataset\u001b[39;00m\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m# 初始化 Trainer\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mtrainer = Trainer(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03mtrainer.train()\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义 GPT-2 微调器类\n",
    "class GPT2FineTuner:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        # 设置 pad_token 为 eos_token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        # 对文本进行编码，并设置 labels 与 input_ids 相同\n",
    "        encoding = self.tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].copy()  # 设置 labels\n",
    "        return encoding\n",
    "'''\n",
    "# 加载和处理数据集\n",
    "def load_and_prepare_data():\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": \"prepared_conversations.txt\"})\n",
    "    return dataset[\"train\"]\n",
    "'''\n",
    "def load_and_prepare_data(finetuner, batch_size=16, num_workers=4):\n",
    "    # 加载数据集\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": \"prepared_conversations.txt\"})[\"train\"]\n",
    "    # 处理数据\n",
    "    tokenized_dataset = dataset.map(finetuner.tokenize_function, batched=True)\n",
    "\n",
    "    # 使用 DataLoader 来加载数据集\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=lambda x: {\n",
    "            'input_ids': torch.tensor([f['input_ids'] for f in x], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([f['attention_mask'] for f in x], dtype=torch.long),\n",
    "            'labels': torch.tensor([f['input_ids'] for f in x], dtype=torch.long)  # 设置 labels\n",
    "        }\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "finetuner = GPT2FineTuner(model_name)\n",
    "# 调用函数加载数据\n",
    "dataloader = load_and_prepare_data(finetuner, batch_size=16, num_workers=4)\n",
    "#tokenized_dataset = dataset.map(finetuner.tokenize_function, batched=True)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetune_gpt2\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",  # 日志保存目录\n",
    "    save_steps=500,  # 每 500 步保存一次模型\n",
    "    logging_steps=100,  # 每 100 步打印日志\n",
    ")\n",
    "# 自定义 Trainer 类来支持自定义 DataLoader\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return dataloader\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=finetuner.model,\n",
    "    args=training_args,\n",
    "    train_dataset=None,  # 因为我们重写了 get_train_dataloader，不需要传递 train_dataset\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "'''\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=finetuner.model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\\nfrom torch.utils.data import DataLoader\\nimport torch\\nfrom datasets import load_dataset\\nfrom torch.optim import AdamW\\n\\n# 定义 GPT-2 微调器类\\nclass GPT2FineTuner:\\n    def __init__(self, model_name):\\n        self.model = GPT2LMHeadModel.from_pretrained(model_name)\\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\\n        self.tokenizer.pad_token = self.tokenizer.eos_token\\n\\n    def tokenize_function(self, examples):\\n        encoding = self.tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\\n        encoding[\"labels\"] = encoding[\"input_ids\"].copy()\\n        return encoding\\n\\n# 加载和处理数据集，并使用 DataLoader\\ndef load_and_prepare_data(batch_size=4, num_workers=0):\\n    # 加载数据集\\n    dataset = load_dataset(\"text\", data_files={\"train\": \"prepared_conversations.txt\"})[\"train\"]\\n    # 处理数据\\n    tokenized_dataset = dataset.map(finetuner.tokenize_function, batched=True)\\n    \\n    # 使用 DataLoader 来加载数据集\\n    dataloader = DataLoader(\\n        tokenized_dataset,\\n        batch_size=batch_size,\\n        shuffle=True,\\n        num_workers=num_workers,\\n        collate_fn=lambda x: {\\n            \\'input_ids\\': torch.tensor([f[\\'input_ids\\'] for f in x], dtype=torch.long),\\n            \\'attention_mask\\': torch.tensor([f[\\'attention_mask\\'] for f in x], dtype=torch.long),\\n            \\'labels\\': torch.tensor([f[\\'labels\\'] for f in x], dtype=torch.long)\\n        }\\n    )\\n    return dataloader\\n\\n# 加载模型\\nmodel_name = \"gpt2\"\\nfinetuner = GPT2FineTuner(model_name)\\ndataloader = load_and_prepare_data(batch_size=16, num_workers=2)\\n\\n# 训练设置\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nfinetuner.model.to(device)\\noptimizer = AdamW(finetuner.model.parameters(), lr=5e-5)\\n\\n# 自定义训练循环\\nfor epoch in range(1):  # 设置 epoch 数量\\n    finetuner.model.train()\\n    for batch in dataloader:\\n        # 将 batch 数据传到 GPU（如果可用）\\n        input_ids = batch[\\'input_ids\\'].to(device)\\n        attention_mask = batch[\\'attention_mask\\'].to(device)\\n        labels = batch[\\'labels\\'].to(device)\\n\\n        # 前向传播\\n        outputs = finetuner.model(input_ids, attention_mask=attention_mask, labels=labels)\\n        loss = outputs.loss\\n\\n        # 反向传播和优化\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 定义 GPT-2 微调器类\n",
    "class GPT2FineTuner:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        encoding = self.tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
    "        return encoding\n",
    "\n",
    "# 加载和处理数据集，并使用 DataLoader\n",
    "def load_and_prepare_data(batch_size=4, num_workers=0):\n",
    "    # 加载数据集\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": \"prepared_conversations.txt\"})[\"train\"]\n",
    "    # 处理数据\n",
    "    tokenized_dataset = dataset.map(finetuner.tokenize_function, batched=True)\n",
    "    \n",
    "    # 使用 DataLoader 来加载数据集\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=lambda x: {\n",
    "            'input_ids': torch.tensor([f['input_ids'] for f in x], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([f['attention_mask'] for f in x], dtype=torch.long),\n",
    "            'labels': torch.tensor([f['labels'] for f in x], dtype=torch.long)\n",
    "        }\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "# 加载模型\n",
    "model_name = \"gpt2\"\n",
    "finetuner = GPT2FineTuner(model_name)\n",
    "dataloader = load_and_prepare_data(batch_size=16, num_workers=2)\n",
    "\n",
    "# 训练设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuner.model.to(device)\n",
    "optimizer = AdamW(finetuner.model.parameters(), lr=5e-5)\n",
    "\n",
    "# 自定义训练循环\n",
    "for epoch in range(1):  # 设置 epoch 数量\n",
    "    finetuner.model.train()\n",
    "    for batch in dataloader:\n",
    "        # 将 batch 数据传到 GPU（如果可用）\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = finetuner.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from Pre-trained Model:\n",
      "How are you doing today?\n",
      "\n",
      "I'm doing a lot of work. I'm working on a new book. And I've got a couple of new projects coming up.\n",
      "...\n",
      " (Laughs.)\n",
      "\n",
      "\n",
      "\n",
      "What's your\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./unsloth_gpt2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 加载微调后的模型\u001b[39;00m\n\u001b[1;32m     27\u001b[0m finetuned_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./unsloth_gpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinetuned_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m finetuned_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(finetuned_model_path)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 生成微调后模型的输出\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/finetune/lib/python3.10/site-packages/transformers/modeling_utils.py:3504\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3500\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3501\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3502\u001b[0m         )\n\u001b[1;32m   3503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3505\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3506\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3507\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3508\u001b[0m         )\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   3510\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./unsloth_gpt2."
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# 1. 加载预训练的 GPT-2 模型\n",
    "pretrained_model_name = \"gpt2\"\n",
    "pretrained_tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name)\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# 生成预训练模型的输出\n",
    "prompt = \"How are you doing today?\"\n",
    "pretrained_output = generate_text(pretrained_model, pretrained_tokenizer, prompt)\n",
    "print(\"Output from Pre-trained Model:\")\n",
    "print(pretrained_output)\n",
    "\n",
    "# 加载微调后的模型\n",
    "finetuned_model_path = \"./unsloth_gpt2\"\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path)\n",
    "finetuned_tokenizer = GPT2Tokenizer.from_pretrained(finetuned_model_path)\n",
    "\n",
    "# 生成微调后模型的输出\n",
    "finetuned_output = generate_text(finetuned_model, finetuned_tokenizer, prompt)\n",
    "print(\"\\nOutput from Fine-tuned Model:\")\n",
    "print(finetuned_output)\n",
    "\n",
    "# 3. 对比两个模型的输出\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Pre-trained Output:\", pretrained_output)\n",
    "print(\"Fine-tuned Output:\", finetuned_output)\n",
    "\n",
    "'''\n",
    "# 输入初始对话文本\n",
    "input_text = \"How are you doing today?\"\n",
    "\n",
    "# 对输入进行编码\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 生成对话\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling, TFGPT2LMHeadModel\\nimport torch\\nfrom unsloth import FastLanguageModel\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n# 加载预训练模型和分词器\\nmodel_name = \"gpt2\"\\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\\n#model = TFGPT2LMHeadModel.from_pretrained(model_name)\\nmodel.to(device)\\n\\n# 加载和准备数据集\\ndef load_dataset(file_path, tokenizer, block_size=128):\\n    return TextDataset(\\n        tokenizer=tokenizer,\\n        file_path=file_path,\\n        block_size=block_size,\\n    )\\n\\ndef data_collator(tokenizer):\\n    return DataCollatorForLanguageModeling(\\n        tokenizer=tokenizer,\\n        mlm=False,\\n    )\\n\\ndataset = load_dataset(\"prepared_conversations.txt\", tokenizer)\\ncollator = data_collator(tokenizer)\\n\\n# 设置训练参数\\ntraining_args = TrainingArguments(\\n    output_dir=\"./gpt2-finetuned\",\\n    overwrite_output_dir=True,\\n    num_train_epochs=3,\\n    per_device_train_batch_size=4,\\n    save_steps=10_000,\\n    save_total_limit=2,\\n)\\n\\n# 进行训练\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    data_collator=collator,\\n    train_dataset=dataset,\\n)\\n\\ntrainer.train()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling, TFGPT2LMHeadModel\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "#model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# 加载和准备数据集\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size,\n",
    "    )\n",
    "\n",
    "def data_collator(tokenizer):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "dataset = load_dataset(\"prepared_conversations.txt\", tokenizer)\n",
    "collator = data_collator(tokenizer)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# 进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
