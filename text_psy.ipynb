{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tr\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "# è®¾ç½® PYTORCH_CUDA_ALLOC_CONF ä»¥é¿å…å†…å­˜ç¢ç‰‡åŒ–\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®å·²æˆåŠŸè½¬æ¢å¹¶ä¿å­˜ä¸º train.txt\n"
     ]
    }
   ],
   "source": [
    "def prepare_training_data(json_file, output_file):\n",
    "    \"\"\"å°† JSON æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒç”¨çš„çº¯æ–‡æœ¬æ ¼å¼\"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            # æå–é—®é¢˜éƒ¨åˆ†ï¼Œå¹¶å»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œç¬¦\n",
    "            question = entry[\"content\"].replace(\"\\n\", \" \").strip()\n",
    "            \n",
    "            # è¿æ¥æ‰€æœ‰å›å¤ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¿‡æ»¤ç©ºå›å¤\n",
    "            responses = [resp.strip() for resp in entry[\"reponses\"] if resp.strip()]\n",
    "            answer = \" \".join(responses)\n",
    "\n",
    "            # å¦‚æœé—®é¢˜æˆ–ç­”æ¡ˆä¸ºç©ºï¼Œåˆ™è·³è¿‡è¯¥æ¡æ•°æ®\n",
    "            if not question or not answer:\n",
    "                continue\n",
    "\n",
    "            # å†™å…¥æ–‡ä»¶\n",
    "            f.write(f\"Q: {question}\\nA: {answer}\\n\\n\")\n",
    "\n",
    "# å°† JSON æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ä¾›è®­ç»ƒä½¿ç”¨\n",
    "prepare_training_data(\"./ptt_articles.json\", \"train.txt\")\n",
    "print(\"æ•°æ®å·²æˆåŠŸè½¬æ¢å¹¶ä¿å­˜ä¸º train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom unsloth import FastLanguageModel\\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset\\n\\n# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n# åŠ è½½ LLaMA-1B æ¨¡å‹å¹¶å¯ç”¨ 4-bit å’Œ FP16\\nmodel, tokenizer = FastLanguageModel.from_pretrained(\\n    model_name=\"unsloth/Llama-3.2-1B-Instruct\",\\n    device_map={\"\": \"cuda\"},       # å°†æ¨¡å‹æ‰€æœ‰æ¨¡å—åŠ è½½åˆ° GPU ä¸Š\\n    dtype=torch.float16,      # ä½¿ç”¨åŠç²¾åº¦ FP16ï¼Œå‡å°‘æ˜¾å­˜å ç”¨\\n    max_length=30,\\n    load_in_4bit=True,             # å¯ç”¨ 4-bit é‡åŒ–\\n)\\nmodel.to(device)\\n\\n# åŠ è½½è®­ç»ƒæ•°æ®é›†\\ndef load_dataset(file_path, block_size=512):\\n    return TextDataset(\\n        tokenizer=tokenizer,\\n        file_path=file_path,\\n        block_size=block_size\\n    )\\n\\ntrain_dataset = load_dataset(\"train.txt\")\\n\\n# åˆ›å»ºæ•°æ®æ‰¹å¤„ç†å™¨\\ndata_collator = DataCollatorForLanguageModeling(\\n    tokenizer=tokenizer, mlm=False  # LLaMA æ˜¯è‡ªå›å½’æ¨¡å‹\\n)\\n\\n# è®¾ç½®è®­ç»ƒå‚æ•°\\ntraining_args = TrainingArguments(\\n    output_dir=\"./finetuned_llama\",\\n    overwrite_output_dir=True,\\n    num_train_epochs=3,                  # è®­ç»ƒè½®æ•°\\n    per_device_train_batch_size=1,       # å°æ‰¹æ¬¡ï¼Œé˜²æ­¢ OOM\\n    gradient_accumulation_steps=8,       # æ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡\\n    learning_rate=5e-5,\\n    logging_steps=100,\\n    save_steps=500,\\n    save_total_limit=2,\\n)\\n\\n# åˆå§‹åŒ– Trainer\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    data_collator=data_collator,\\n    train_dataset=train_dataset,\\n)\\n\\n# å¼€å§‹å¾®è°ƒ\\nprint(\"å¼€å§‹å¾®è°ƒæ¨¡å‹...\")\\ntrainer.train()\\nprint(\"æ¨¡å‹å¾®è°ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼\")\\n\\n# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨\\ntrainer.save_model(\"./finetuned_llama\")\\ntokenizer.save_pretrained(\"./finetuned_llama\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset\n",
    "\n",
    "# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# åŠ è½½ LLaMA-1B æ¨¡å‹å¹¶å¯ç”¨ 4-bit å’Œ FP16\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    device_map={\"\": \"cuda\"},       # å°†æ¨¡å‹æ‰€æœ‰æ¨¡å—åŠ è½½åˆ° GPU ä¸Š\n",
    "    dtype=torch.float16,      # ä½¿ç”¨åŠç²¾åº¦ FP16ï¼Œå‡å°‘æ˜¾å­˜å ç”¨\n",
    "    max_length=30,\n",
    "    load_in_4bit=True,             # å¯ç”¨ 4-bit é‡åŒ–\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# åŠ è½½è®­ç»ƒæ•°æ®é›†\n",
    "def load_dataset(file_path, block_size=512):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset(\"train.txt\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®æ‰¹å¤„ç†å™¨\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # LLaMA æ˜¯è‡ªå›å½’æ¨¡å‹\n",
    ")\n",
    "\n",
    "# è®¾ç½®è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_llama\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                  # è®­ç»ƒè½®æ•°\n",
    "    per_device_train_batch_size=1,       # å°æ‰¹æ¬¡ï¼Œé˜²æ­¢ OOM\n",
    "    gradient_accumulation_steps=8,       # æ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ– Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# å¼€å§‹å¾®è°ƒ\n",
    "print(\"å¼€å§‹å¾®è°ƒæ¨¡å‹...\")\n",
    "trainer.train()\n",
    "print(\"æ¨¡å‹å¾®è°ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼\")\n",
    "\n",
    "# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "trainer.save_model(\"./finetuned_llama\")\n",
    "tokenizer.save_pretrained(\"./finetuned_llama\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2650019ce35f4b5d8ad921148c4a3598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaia/miniforge3/envs/TR/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# ä½¿ç”¨ GPT2 çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œé€‚ç”¨äºä¸­æ–‡\n",
    "model_name = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "\n",
    "# åŠ è½½ GPT-2 æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "#tokenizer(return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "# åŠ è½½ GPT-2 åˆ†è¯å™¨\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # è®¾ç½® EOS ä¸º padding token\n",
    "\n",
    "def load_dataset(file_path, block_size=512):\n",
    "    \"\"\"åŠ è½½æ–‡æœ¬æ•°æ®é›†å¹¶è¿›è¡Œåˆ†å—å¤„ç†\"\"\"\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size  # æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§é•¿åº¦\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset(\"train.txt\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # GPT-2 ä¸ä½¿ç”¨ MLM ä»»åŠ¡\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_gpt2\",   # æ¨¡å‹ä¿å­˜è·¯å¾„\n",
    "    overwrite_output_dir=True,       # è¦†ç›–ç°æœ‰æ–‡ä»¶\n",
    "    num_train_epochs=3,              # è®­ç»ƒè½®æ•°\n",
    "    per_device_train_batch_size=1,   # æ¯ä¸ªè®¾å¤‡çš„æ‰¹å¤§å°\n",
    "    gradient_accumulation_steps=8,   # æ¢¯åº¦ç´¯ç§¯\n",
    "    learning_rate=5e-5,              # å­¦ä¹ ç‡\n",
    "    logging_dir=\"./logs\",            # æ—¥å¿—è·¯å¾„\n",
    "    logging_steps=100,               # æ¯100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
    "    save_steps=500,                  # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    save_total_limit=2,              # æœ€å¤šä¿å­˜2ä¸ªæ£€æŸ¥ç‚¹\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 æ¨¡å‹\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "\n",
    "# åˆå§‹åŒ– Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹å¾®è°ƒæ¨¡å‹...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038131e087784b49add7ad1d3d1da2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10.9012, 'train_samples_per_second': 17.062, 'train_steps_per_second': 1.926, 'train_loss': 3.4956483386811756, 'epoch': 2.71}\n",
      "å¾®è°ƒå®Œæˆï¼\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_gpt2/tokenizer_config.json',\n",
       " './finetuned_gpt2/special_tokens_map.json',\n",
       " './finetuned_gpt2/vocab.txt',\n",
       " './finetuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"å¼€å§‹å¾®è°ƒæ¨¡å‹...\")\n",
    "trainer.train()\n",
    "print(\"å¾®è°ƒå®Œæˆï¼\")\n",
    "\n",
    "# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "trainer.save_model(\"./finetuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./finetuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21128, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "original_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(\"./finetuned_gpt2\")\n",
    "finetuned_tokenizer = BertTokenizer.from_pretrained(\"./finetuned_gpt2\")\n",
    "\n",
    "# å°†æ¨¡å‹ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)\n",
    "finetuned_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt):##, max_length=256\n",
    "    \"\"\"ç”Ÿæˆæ–‡æœ¬çš„å‡½æ•°\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    #input_ids = inputs['input_ids']\n",
    "    #attention_mask = input['attention_mask']\n",
    "    #print(inputs)\n",
    "    #print(inputs[\"input_ids\"][0])\n",
    "    with torch.no_grad():\n",
    "        '''\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,      # ä½¿ç”¨éšæœºé‡‡æ ·ç”Ÿæˆæ›´æœ‰è¶£çš„ç»“æœ\n",
    "            top_k=50,            # é™åˆ¶æœ€é«˜ k ä¸ªå€™é€‰è¯\n",
    "            top_p=0.95           # ä½¿ç”¨ nucleus sampling ç”Ÿæˆ\n",
    "        )\n",
    "        '''\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # æ§åˆ¶ç”Ÿæˆçš„éš¨æ©Ÿæ€§\n",
    "        max_new_tokens=50,  # æ§åˆ¶ç”Ÿæˆé•·åº¦\n",
    "        top_p=0.9,  # ä½¿ç”¨ nucleus sampling\n",
    "        top_k=50,  # è€ƒæ…®å‰ 50 å€‹ token\n",
    "        repetition_penalty=2.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # å¡«å…… token\n",
    "        eos_token_id=tokenizer.eos_token_id,  # çµæŸ token\n",
    "        num_return_sequences=1\n",
    "        #clean_up_tokenization_spaces=True   #if use generator in pipeline or mayber gpt-4 than set the value\n",
    "        )\n",
    "    # ç¡®ä¿ output æ˜¯æ•´æ•°åˆ—è¡¨\n",
    "    token_ids = output[0].tolist()\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    #return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾®è°ƒå‰çš„è¾“å‡ºï¼š\n",
      "original_output:è«‹å•æœ‰äººåƒéæ¨‚å¾©å¾—ä¾†æ²»ç™‚å¼·è¿«ç—‡å— æˆ–è€…å› å…¶å®ƒçš„ç²¾ç¥å®˜èƒ½ç—‡åƒæ¨‚å¾©å¾—æœ‰ä»€éº¼å‰¯ä½œç”¨å— å¯ä»¥è·Ÿæˆ‘å‘ŠçŸ¥ä½ æˆ–ä½ æœ‹å‹çš„å‰¯ä½œç”¨æ˜¯ä»€éº¼å— è¬è¬åäº”å­¦åŠ¡äº¤å¤§å¤šå°±è¯´å·®ä¸æ•™å¯¹ä¸Šè¡Œ,è€³åœ£ç‹ç›¸ä¿¢å¿…è¦ªæ­ä¸€èˆã€‚\n",
      "The fact that\n",
      "\n",
      "å¾®è°ƒåçš„è¾“å‡ºï¼š\n",
      "finetuned_output:è«‹ å• æœ‰ äºº åƒ é æ¨‚ å¾© å¾— ä¾† æ²» ç™‚ å¼· è¿« ç—‡ å— æˆ– è€… å›  å…¶ å®ƒ çš„ ç²¾ ç¥ å®˜ èƒ½ ç—‡ åƒ æ¨‚ å¾© å¾— æœ‰ ä»€ éº¼ å‰¯ ä½œ ç”¨ å— å¯ ä»¥ è·Ÿ æˆ‘ å‘Š çŸ¥ ä½  æˆ– ä½  æœ‹ å‹ çš„ å‰¯ ä½œ ç”¨ æ˜¯ ä»€ éº¼ å— è¬ è¬ ï¼Œ æ‚¨ å¥½ ã€‚ æ²’ äº‹ èªª è©± å¤š äº† å°± æœƒ è¢« ç½µ è¦ ä¸ ç„¶ é‚„ æƒ³ è®“ åˆ¥ çµ¦ ä»– å€‘ é» å€‹ è®š. é€™ æ¨£ : å° æ–¼ ä½• è™• ï¼Ÿ ç„¡ è«– è½ ä¸€ èˆ¬ éƒ½ è¦º è‘— ï¼š æ¯” è¼ƒ å°‘ æ•¸\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(finetuned_output)):##æ²’æœ‰ä¸€æ¬¡generateçš„å—?\\n    #print(output[i].shape)\\n    #generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\\n    generated_text = tokenizer.decode(finetuned_output[i], skip_special_tokens=True)\\n    # on purpose of to make sure that a sentences end in some certain  Marks.\\n    if not generated_text[-1] in \".!?\":\\n        last_punctuation = max(generated_text.rfind(p) for p in \".!?\")\\n        if last_punctuation != -1:\\n            generated_text = generated_text[:last_punctuation + 1]\\n    print(f\"{i} sentences: {generated_text}\")\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# å®šä¹‰æµ‹è¯•æç¤º\n",
    "prompt = \"è«‹å•æœ‰äººåƒéæ¨‚å¾©å¾—ä¾†æ²»ç™‚å¼·è¿«ç—‡å— æˆ–è€…å› å…¶å®ƒçš„ç²¾ç¥å®˜èƒ½ç—‡åƒæ¨‚å¾©å¾—æœ‰ä»€éº¼å‰¯ä½œç”¨å— å¯ä»¥è·Ÿæˆ‘å‘ŠçŸ¥ä½ æˆ–ä½ æœ‹å‹çš„å‰¯ä½œç”¨æ˜¯ä»€éº¼å— è¬è¬\"\n",
    "\n",
    "# å¾®è°ƒå‰çš„è¾“å‡º\n",
    "print(\"å¾®è°ƒå‰çš„è¾“å‡ºï¼š\")\n",
    "original_output = generate_response(original_model, original_tokenizer, prompt)\n",
    "print(f'original_output:{original_output}')\n",
    "#print(original_output)\n",
    "'''\n",
    "for i in range(len(original_output)):##æ²’æœ‰ä¸€æ¬¡generateçš„å—?\n",
    "    #print(output[i].shape)\n",
    "    #generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    generated_text = tokenizer.decode(original_output[i], skip_special_tokens=True)\n",
    "    # on purpose of to make sure that a sentences end in some certain  Marks.\n",
    "    if not generated_text[-1] in \".!?\":\n",
    "        last_punctuation = max(generated_text.rfind(p) for p in \".!?\")\n",
    "        if last_punctuation != -1:\n",
    "            generated_text = generated_text[:last_punctuation + 1]\n",
    "    print(f\"{i} sentences: {generated_text}\")\n",
    "'''\n",
    "# å¾®è°ƒåçš„è¾“å‡º\n",
    "print(\"\\nå¾®è°ƒåçš„è¾“å‡ºï¼š\")\n",
    "finetuned_output = generate_response(finetuned_model, finetuned_tokenizer, prompt)\n",
    "print(f'finetuned_output:{finetuned_output}')\n",
    "'''\n",
    "for i in range(len(finetuned_output)):##æ²’æœ‰ä¸€æ¬¡generateçš„å—?\n",
    "    #print(output[i].shape)\n",
    "    #generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    generated_text = tokenizer.decode(finetuned_output[i], skip_special_tokens=True)\n",
    "    # on purpose of to make sure that a sentences end in some certain  Marks.\n",
    "    if not generated_text[-1] in \".!?\":\n",
    "        last_punctuation = max(generated_text.rfind(p) for p in \".!?\")\n",
    "        if last_punctuation != -1:\n",
    "            generated_text = generated_text[:last_punctuation + 1]\n",
    "    print(f\"{i} sentences: {generated_text}\")\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
