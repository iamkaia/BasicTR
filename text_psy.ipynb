{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tr\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "# 设置 PYTORCH_CUDA_ALLOC_CONF 以避免内存碎片化\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功转换并保存为 train.txt\n"
     ]
    }
   ],
   "source": [
    "def prepare_training_data(json_file, output_file):\n",
    "    \"\"\"将 JSON 数据转换为训练用的纯文本格式\"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            # 提取问题部分，并去除多余空格和换行符\n",
    "            question = entry[\"content\"].replace(\"\\n\", \" \").strip()\n",
    "            \n",
    "            # 连接所有回复为一个字符串，过滤空回复\n",
    "            responses = [resp.strip() for resp in entry[\"reponses\"] if resp.strip()]\n",
    "            answer = \" \".join(responses)\n",
    "\n",
    "            # 如果问题或答案为空，则跳过该条数据\n",
    "            if not question or not answer:\n",
    "                continue\n",
    "\n",
    "            # 写入文件\n",
    "            f.write(f\"Q: {question}\\nA: {answer}\\n\\n\")\n",
    "\n",
    "# 将 JSON 数据转换为文本格式供训练使用\n",
    "prepare_training_data(\"./ptt_articles.json\", \"train.txt\")\n",
    "print(\"数据已成功转换并保存为 train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom unsloth import FastLanguageModel\\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset\\n\\n# 检查 GPU 是否可用\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n# 加载 LLaMA-1B 模型并启用 4-bit 和 FP16\\nmodel, tokenizer = FastLanguageModel.from_pretrained(\\n    model_name=\"unsloth/Llama-3.2-1B-Instruct\",\\n    device_map={\"\": \"cuda\"},       # 将模型所有模块加载到 GPU 上\\n    dtype=torch.float16,      # 使用半精度 FP16，减少显存占用\\n    max_length=30,\\n    load_in_4bit=True,             # 启用 4-bit 量化\\n)\\nmodel.to(device)\\n\\n# 加载训练数据集\\ndef load_dataset(file_path, block_size=512):\\n    return TextDataset(\\n        tokenizer=tokenizer,\\n        file_path=file_path,\\n        block_size=block_size\\n    )\\n\\ntrain_dataset = load_dataset(\"train.txt\")\\n\\n# 创建数据批处理器\\ndata_collator = DataCollatorForLanguageModeling(\\n    tokenizer=tokenizer, mlm=False  # LLaMA 是自回归模型\\n)\\n\\n# 设置训练参数\\ntraining_args = TrainingArguments(\\n    output_dir=\"./finetuned_llama\",\\n    overwrite_output_dir=True,\\n    num_train_epochs=3,                  # 训练轮数\\n    per_device_train_batch_size=1,       # 小批次，防止 OOM\\n    gradient_accumulation_steps=8,       # 梯度累积，模拟更大批次\\n    learning_rate=5e-5,\\n    logging_steps=100,\\n    save_steps=500,\\n    save_total_limit=2,\\n)\\n\\n# 初始化 Trainer\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    data_collator=data_collator,\\n    train_dataset=train_dataset,\\n)\\n\\n# 开始微调\\nprint(\"开始微调模型...\")\\ntrainer.train()\\nprint(\"模型微调完成并已保存！\")\\n\\n# 保存微调后的模型和分词器\\ntrainer.save_model(\"./finetuned_llama\")\\ntokenizer.save_pretrained(\"./finetuned_llama\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载 LLaMA-1B 模型并启用 4-bit 和 FP16\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    device_map={\"\": \"cuda\"},       # 将模型所有模块加载到 GPU 上\n",
    "    dtype=torch.float16,      # 使用半精度 FP16，减少显存占用\n",
    "    max_length=30,\n",
    "    load_in_4bit=True,             # 启用 4-bit 量化\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# 加载训练数据集\n",
    "def load_dataset(file_path, block_size=512):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset(\"train.txt\")\n",
    "\n",
    "# 创建数据批处理器\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # LLaMA 是自回归模型\n",
    ")\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_llama\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                  # 训练轮数\n",
    "    per_device_train_batch_size=1,       # 小批次，防止 OOM\n",
    "    gradient_accumulation_steps=8,       # 梯度累积，模拟更大批次\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# 开始微调\n",
    "print(\"开始微调模型...\")\n",
    "trainer.train()\n",
    "print(\"模型微调完成并已保存！\")\n",
    "\n",
    "# 保存微调后的模型和分词器\n",
    "trainer.save_model(\"./finetuned_llama\")\n",
    "tokenizer.save_pretrained(\"./finetuned_llama\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2650019ce35f4b5d8ad921148c4a3598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaia/miniforge3/envs/TR/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling, GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# 使用 GPT2 的快速分词器，适用于中文\n",
    "model_name = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "\n",
    "# 加载 GPT-2 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "#tokenizer(return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "# 加载 GPT-2 分词器\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 设置 EOS 为 padding token\n",
    "\n",
    "def load_dataset(file_path, block_size=512):\n",
    "    \"\"\"加载文本数据集并进行分块处理\"\"\"\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size  # 每个样本的最大长度\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset(\"train.txt\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # GPT-2 不使用 MLM 任务\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_gpt2\",   # 模型保存路径\n",
    "    overwrite_output_dir=True,       # 覆盖现有文件\n",
    "    num_train_epochs=3,              # 训练轮数\n",
    "    per_device_train_batch_size=1,   # 每个设备的批大小\n",
    "    gradient_accumulation_steps=8,   # 梯度累积\n",
    "    learning_rate=5e-5,              # 学习率\n",
    "    logging_dir=\"./logs\",            # 日志路径\n",
    "    logging_steps=100,               # 每100步记录一次日志\n",
    "    save_steps=500,                  # 每500步保存一次模型\n",
    "    save_total_limit=2,              # 最多保存2个检查点\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer\n",
    "\n",
    "# 加载预训练的 GPT-2 模型\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始微调模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038131e087784b49add7ad1d3d1da2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10.9012, 'train_samples_per_second': 17.062, 'train_steps_per_second': 1.926, 'train_loss': 3.4956483386811756, 'epoch': 2.71}\n",
      "微调完成！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_gpt2/tokenizer_config.json',\n",
       " './finetuned_gpt2/special_tokens_map.json',\n",
       " './finetuned_gpt2/vocab.txt',\n",
       " './finetuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"开始微调模型...\")\n",
    "trainer.train()\n",
    "print(\"微调完成！\")\n",
    "\n",
    "# 保存微调后的模型和分词器\n",
    "trainer.save_model(\"./finetuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./finetuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21128, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预训练模型和分词器\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "original_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 加载微调后的模型和分词器\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(\"./finetuned_gpt2\")\n",
    "finetuned_tokenizer = BertTokenizer.from_pretrained(\"./finetuned_gpt2\")\n",
    "\n",
    "# 将模型移动到 GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "original_model.to(device)\n",
    "finetuned_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt):##, max_length=256\n",
    "    \"\"\"生成文本的函数\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    #input_ids = inputs['input_ids']\n",
    "    #attention_mask = input['attention_mask']\n",
    "    #print(inputs)\n",
    "    #print(inputs[\"input_ids\"][0])\n",
    "    with torch.no_grad():\n",
    "        '''\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,      # 使用随机采样生成更有趣的结果\n",
    "            top_k=50,            # 限制最高 k 个候选词\n",
    "            top_p=0.95           # 使用 nucleus sampling 生成\n",
    "        )\n",
    "        '''\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # 控制生成的隨機性\n",
    "        max_new_tokens=50,  # 控制生成長度\n",
    "        top_p=0.9,  # 使用 nucleus sampling\n",
    "        top_k=50,  # 考慮前 50 個 token\n",
    "        repetition_penalty=2.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # 填充 token\n",
    "        eos_token_id=tokenizer.eos_token_id,  # 結束 token\n",
    "        num_return_sequences=1\n",
    "        #clean_up_tokenization_spaces=True   #if use generator in pipeline or mayber gpt-4 than set the value\n",
    "        )\n",
    "    # 确保 output 是整数列表\n",
    "    token_ids = output[0].tolist()\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    #return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微调前的输出：\n",
      "original_output:請問有人吃過樂復得來治療強迫症嗎 或者因其它的精神官能症吃樂復得有什麼副作用嗎 可以跟我告知你或你朋友的副作用是什麼嗎 謝謝十五学务交大多就说差不教对上行,耳圣王相俢必親歐一舞。\n",
      "The fact that\n",
      "\n",
      "微调后的输出：\n",
      "finetuned_output:請 問 有 人 吃 過 樂 復 得 來 治 療 強 迫 症 嗎 或 者 因 其 它 的 精 神 官 能 症 吃 樂 復 得 有 什 麼 副 作 用 嗎 可 以 跟 我 告 知 你 或 你 朋 友 的 副 作 用 是 什 麼 嗎 謝 謝 ， 您 好 。 沒 事 說 話 多 了 就 會 被 罵 要 不 然 還 想 讓 別 給 他 們 點 個 讚. 這 樣 : 對 於 何 處 ？ 無 論 聽 一 般 都 覺 著 ： 比 較 少 數\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(finetuned_output)):##沒有一次generate的嗎?\\n    #print(output[i].shape)\\n    #generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\\n    generated_text = tokenizer.decode(finetuned_output[i], skip_special_tokens=True)\\n    # on purpose of to make sure that a sentences end in some certain  Marks.\\n    if not generated_text[-1] in \".!?\":\\n        last_punctuation = max(generated_text.rfind(p) for p in \".!?\")\\n        if last_punctuation != -1:\\n            generated_text = generated_text[:last_punctuation + 1]\\n    print(f\"{i} sentences: {generated_text}\")\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# 定义测试提示\n",
    "prompt = \"請問有人吃過樂復得來治療強迫症嗎 或者因其它的精神官能症吃樂復得有什麼副作用嗎 可以跟我告知你或你朋友的副作用是什麼嗎 謝謝\"\n",
    "\n",
    "# 微调前的输出\n",
    "print(\"微调前的输出：\")\n",
    "original_output = generate_response(original_model, original_tokenizer, prompt)\n",
    "print(f'original_output:{original_output}')\n",
    "#print(original_output)\n",
    "'''\n",
    "for i in range(len(original_output)):##沒有一次generate的嗎?\n",
    "    #print(output[i].shape)\n",
    "    #generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    generated_text = tokenizer.decode(original_output[i], skip_special_tokens=True)\n",
    "    # on purpose of to make sure that a sentences end in some certain  Marks.\n",
    "    if not generated_text[-1] in \".!?\":\n",
    "        last_punctuation = max(generated_text.rfind(p) for p in \".!?\")\n",
    "        if last_punctuation != -1:\n",
    "            generated_text = generated_text[:last_punctuation + 1]\n",
    "    print(f\"{i} sentences: {generated_text}\")\n",
    "'''\n",
    "# 微调后的输出\n",
    "print(\"\\n微调后的输出：\")\n",
    "finetuned_output = generate_response(finetuned_model, finetuned_tokenizer, prompt)\n",
    "print(f'finetuned_output:{finetuned_output}')\n",
    "'''\n",
    "for i in range(len(finetuned_output)):##沒有一次generate的嗎?\n",
    "    #print(output[i].shape)\n",
    "    #generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    generated_text = tokenizer.decode(finetuned_output[i], skip_special_tokens=True)\n",
    "    # on purpose of to make sure that a sentences end in some certain  Marks.\n",
    "    if not generated_text[-1] in \".!?\":\n",
    "        last_punctuation = max(generated_text.rfind(p) for p in \".!?\")\n",
    "        if last_punctuation != -1:\n",
    "            generated_text = generated_text[:last_punctuation + 1]\n",
    "    print(f\"{i} sentences: {generated_text}\")\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
